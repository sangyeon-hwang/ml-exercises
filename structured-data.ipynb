{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"https://www.tensorflow.org/alpha/tutorials/keras/feature_columns\"\"\"\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Prepare a CSV data.\n",
    "dataURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\n",
    "dataframe = pd.read_csv(dataURL)  # pandas.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The created `pandas.DataFrame` object has the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape: (303, 14)\n",
      "Attributes: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
      "Column type: <class 'pandas.core.series.Series'>\n",
      "Column size: (303,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Type:\", type(dataframe))\n",
    "print(\"Shape:\", dataframe.shape)\n",
    "print(\"Attributes:\", list(dataframe.keys()))\n",
    "print(\"Column type:\", type(dataframe['age']))\n",
    "print(\"Column size:\", dataframe['age'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each *row* corresponds to a patient (or a data point), and each *column* corresponds to an attribute.\n",
    "\n",
    "Note that column values can be accessed by giving a column name as either an *attribute* or a *key*, i.e., `dataframe.age` or `dataframe['age']` for the age values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataframe into sub-dataframes for training, validating and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193, 14)\n",
      "(49, 14)\n",
      "(61, 14)\n"
     ]
    }
   ],
   "source": [
    "trainFrame, testFrame = train_test_split(dataframe, test_size=0.2)\n",
    "trainFrame, validateFrame = train_test_split(trainFrame, test_size=0.2)\n",
    "print(trainFrame.shape)\n",
    "print(validateFrame.shape)\n",
    "print(testFrame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we wrap each (sub-)dataframe into a `tensorflow.data.Dataset` object. The latter becomes a bridge that maps the dataframe to feature columns, which will be used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe2dataset(dataframe, shuffle=True, batchSize=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('target')  # 1,0-diagnosis of hear disease.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "        # dict(dataframe).keys() -> the data attributes.\n",
    "        # dict(dataframe).values() -> the data values.\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataframe))\n",
    "    dataset = dataset.batch(batchSize)  # Dataset -> BatchDataset\n",
    "    return dataset\n",
    "\n",
    "batchSize = 5  # A small batch size for demonstration.\n",
    "trainSet = dataframe2dataset(trainFrame, batchSize=batchSize)\n",
    "validateSet = dataframe2dataset(validateFrame, False, batchSize)\n",
    "testSet = dataframe2dataset(testFrame, False, batchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`trainSet`, `validateSet` and `testSet` are `BatchDataset` objects. When iterated, they give one **batch** of data rows. Each batch is a tuple of a *feature batch* and a *label batch*. The feature batch is a dict mapping the column names to values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type and length: <class 'tuple'> , 2\n",
      "batch[0] keys: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
      "batch[1] value example: tf.Tensor([42 52 67 50 54], shape=(5,), dtype=int32)\n",
      "batch[1]: tf.Tensor([0 0 1 0 1], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "exampleBatch = next(iter(trainSet))\n",
    "print(\"Type and length:\", type(exampleBatch), \",\", len(exampleBatch))\n",
    "print(\"batch[0] keys:\", list(exampleBatch[0].keys()))\n",
    "print(\"batch[1] value example:\", exampleBatch[0]['age'])\n",
    "print(\"batch[1]:\", exampleBatch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original data has different types of features, e.g., numerical, categorical or binary. `tensorflow.feature_column` provides various types of feature columns.\n",
    "\n",
    "We will use the following helper function to see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(featureColumn):\n",
    "    \"\"\"A utility function to see how a feature batch is transformed\n",
    "       to a feature column.\"\"\"\n",
    "    # First construct a feature layer.\n",
    "    featureLayer = tf.keras.layers.DenseFeatures(featureColumn)\n",
    "    # Provide an example batch to the layer,\n",
    "    transformedBatch = featureLayer(exampleBatch[0])\n",
    "    # and see how the raw input is transformed.\n",
    "    print(transformedBatch.numpy(), \", shape:\", transformedBatch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42.]\n",
      " [52.]\n",
      " [67.]\n",
      " [50.]\n",
      " [54.]] , shape: (5, 1)\n",
      "tf.Tensor([42 52 67 50 54], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "age = tf.feature_column.numeric_column('age')\n",
    "inspect(age)\n",
    "print(exampleBatch[0]['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Bucketized columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]] , shape: (5, 11)\n"
     ]
    }
   ],
   "source": [
    "ageBuckets = tf.feature_column.bucketized_column(\n",
    "    age,\n",
    "    boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65]\n",
    ")\n",
    "inspect(ageBuckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]] , shape: (5, 3)\n",
      "tf.Tensor([b'normal' b'reversible' b'reversible' b'normal' b'normal'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "thal = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'thal', ['fixed', 'normal', 'reversible'])\n",
    "thalOneHot = tf.feature_column.indicator_column(thal)\n",
    "inspect(thalOneHot)\n",
    "print(exampleBatch[0]['thal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Embedding columns.\n",
    "<br>Dense embedding of a categorical one-hot with a large number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.18499786 -0.02930143 -0.0795497   0.4397144   0.6425739  -0.1907638\n",
      "  -0.66962767 -0.31330147]\n",
      " [ 0.3440643   0.32039803  0.35175878 -0.30920753  0.5091526  -0.01856529\n",
      "   0.5360989  -0.55067223]\n",
      " [ 0.3440643   0.32039803  0.35175878 -0.30920753  0.5091526  -0.01856529\n",
      "   0.5360989  -0.55067223]\n",
      " [-0.18499786 -0.02930143 -0.0795497   0.4397144   0.6425739  -0.1907638\n",
      "  -0.66962767 -0.31330147]\n",
      " [-0.18499786 -0.02930143 -0.0795497   0.4397144   0.6425739  -0.1907638\n",
      "  -0.66962767 -0.31330147]] , shape: (5, 8)\n"
     ]
    }
   ],
   "source": [
    "thalEmbedding = tf.feature_column.embedding_column(thal, dimension=8)\n",
    "inspect(thalEmbedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Hashed feature columns.\n",
    "<br>Use `hash_bucket_size` number of hash buckets to encode category strings. `hash_bucket_size` can be much smaller than the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] , shape: (5, 1000)\n"
     ]
    }
   ],
   "source": [
    "thalHashed = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    'thal', hash_bucket_size=1000)\n",
    "inspect(tf.feature_column.indicator_column(thalHashed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Crossed feature columns.\n",
    "<br>Hash encoding of **feature crosses**. The example below crosses the two features, age and thal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] , shape: (5, 1000)\n"
     ]
    }
   ],
   "source": [
    "ageThalCross = tf.feature_column.crossed_column(\n",
    "    [ageBuckets, thal], hash_bucket_size=1000)\n",
    "ageThalOneHot = tf.feature_column.indicator_column(featureCrossExample)\n",
    "inspect(ageThalOneHot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now collect the feature columns that we will use to transform our raw input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureColumns = []\n",
    "# Numeric columns\n",
    "for header in ['age', 'trestbps', 'chol', 'thalach',\n",
    "               'oldpeak', 'slope', 'ca']:\n",
    "    featureColumns.append(tf.feature_column.numeric_column(header))\n",
    "# Bucketized columns\n",
    "featureColumns.append(ageBuckets)\n",
    "# Indicator columns\n",
    "featureColumns.append(thalOneHot)\n",
    "# Embedding columns\n",
    "featureColumns.append(thalEmbedding)\n",
    "# Crossed columns\n",
    "featureColumns.append(ageThalOneHot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the feature columns, we define a feature layer, as done in `inspect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureLayer = tf.keras.layers.DenseFeatures(featureColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we resplit the dataset using a larger batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 32\n",
    "trainSet = dataframe2dataset(trainFrame, batchSize=batchSize)\n",
    "validateSet = dataframe2dataset(validateFrame, False, batchSize)\n",
    "testSet = dataframe2dataset(testFrame, False, batchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define, compile, train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 3.5122 - accuracy: 0.7285 - val_loss: 4.4375 - val_accuracy: 0.7143\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 3.5122 - accuracy: 0.7285 - val_loss: 4.4375 - val_accuracy: 0.7143\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 3.5122 - accuracy: 0.7285 - val_loss: 4.4375 - val_accuracy: 0.7143\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 3.5122 - accuracy: 0.7285 - val_loss: 4.4375 - val_accuracy: 0.7143\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 3.5122 - accuracy: 0.7285 - val_loss: 4.4375 - val_accuracy: 0.7143\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 4.7787 - accuracy: 0.6885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.77874231338501, 0.6885246]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    featureLayer,\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(trainSet,\n",
    "          validation_data=validateSet,\n",
    "          epochs=5)\n",
    "model.evaluate(testSet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
