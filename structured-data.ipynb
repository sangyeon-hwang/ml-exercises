{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"https://www.tensorflow.org/alpha/tutorials/keras/feature_columns\"\"\"\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Prepare a CSV data.\n",
    "dataURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\n",
    "dataframe = pd.read_csv(dataURL)  # pandas.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The created `pandas.DataFrame` object has the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape: (303, 14)\n",
      "Attributes: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
      "Column type: <class 'pandas.core.series.Series'>\n",
      "Column size: (303,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Type:\", type(dataframe))\n",
    "print(\"Shape:\", dataframe.shape)\n",
    "print(\"Attributes:\", list(dataframe.keys()))\n",
    "print(\"Column type:\", type(dataframe['age']))\n",
    "print(\"Column size:\", dataframe['age'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each *row* corresponds to a patient (or a data point), and each *column* corresponds to an attribute.\n",
    "\n",
    "Note that column values can be accessed by giving a column name as either an *attribute* or a *key*, i.e., `dataframe.age` or `dataframe['age']` for the age values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataframe into sub-dataframes for training, validating and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193, 14)\n",
      "(49, 14)\n",
      "(61, 14)\n"
     ]
    }
   ],
   "source": [
    "trainFrame, testFrame = train_test_split(dataframe, test_size=0.2)\n",
    "trainFrame, validateFrame = train_test_split(trainFrame, test_size=0.2)\n",
    "print(trainFrame.shape)\n",
    "print(validateFrame.shape)\n",
    "print(testFrame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we wrap each (sub-)dataframe into a `tensorflow.data.Dataset` object. The latter becomes a bridge that maps the dataframe to feature columns, which will be used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe2dataset(dataframe, shuffle=True, batchSize=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('target')  # 1,0-diagnosis of hear disease.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "        # dict(dataframe).keys() -> the data attributes.\n",
    "        # dict(dataframe).values() -> the data values.\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataframe))\n",
    "    dataset = dataset.batch(batchSize)  # Dataset -> BatchDataset\n",
    "    return dataset\n",
    "\n",
    "batchSize = 5  # A small batch size for demonstration.\n",
    "trainSet = dataframe2dataset(trainFrame, batchSize=batchSize)\n",
    "validateSet = dataframe2dataset(validateFrame, False, batchSize)\n",
    "testSet = dataframe2dataset(testFrame, False, batchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`trainSet`, `validateSet` and `testSet` are `BatchDataset` objects. When iterated, they give one **batch** of data rows. Each batch is a tuple of a *feature batch* and a *label batch*. The feature batch is a dict mapping the column names to values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type and length: <class 'tuple'> , 2\n",
      "batch[0] keys: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
      "batch[1] value example: tf.Tensor([44 43 70 57 40], shape=(5,), dtype=int32)\n",
      "batch[1]: tf.Tensor([0 0 0 0 0], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "exampleBatch = next(iter(trainSet))\n",
    "print(\"Type and length:\", type(exampleBatch), \",\", len(exampleBatch))\n",
    "print(\"batch[0] keys:\", list(exampleBatch[0].keys()))\n",
    "print(\"batch[1] value example:\", exampleBatch[0]['age'])\n",
    "print(\"batch[1]:\", exampleBatch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original data has different types of features, e.g., numerical, categorical or binary. `tensorflow.feature_column` provides various types of feature columns.\n",
    "\n",
    "We will use the following helper function to see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(featureColumn):\n",
    "    \"\"\"A utility function to see how a feature batch is transformed\n",
    "       to a feature column.\"\"\"\n",
    "    featureLayer = tf.keras.layers.DenseFeatures(featureColumn)\n",
    "    transformedBatch = featureLayer(exampleBatch[0])\n",
    "    print(transformedBatch.numpy(), \", shape:\", transformedBatch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.]\n",
      " [43.]\n",
      " [70.]\n",
      " [57.]\n",
      " [40.]] , shape: (5, 1)\n",
      "tf.Tensor([44 43 70 57 40], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "agesExample = tf.feature_column.numeric_column('age')\n",
    "inspect(agesExample)\n",
    "print(exampleBatch[0]['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Bucketized columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] , shape: (5, 11)\n"
     ]
    }
   ],
   "source": [
    "ageBuckets = tf.feature_column.bucketized_column(\n",
    "    agesExample,\n",
    "    boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65]\n",
    ")\n",
    "inspect(ageBuckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]] , shape: (5, 3)\n",
      "tf.Tensor([b'normal' b'normal' b'normal' b'normal' b'reversible'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "thalExample = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'thal', ['fixed', 'normal', 'reversible'])\n",
    "inspect(tf.feature_column.indicator_column(thalExample))\n",
    "print(exampleBatch[0]['thal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Embedding columns.\n",
    "<br>Dense embedding of a categorical one-hot with a large number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.46359766 -0.03813995 -0.06968783 -0.4653723  -0.14574529  0.3870377\n",
      "  -0.65910083  0.1292486 ]\n",
      " [-0.46359766 -0.03813995 -0.06968783 -0.4653723  -0.14574529  0.3870377\n",
      "  -0.65910083  0.1292486 ]\n",
      " [-0.46359766 -0.03813995 -0.06968783 -0.4653723  -0.14574529  0.3870377\n",
      "  -0.65910083  0.1292486 ]\n",
      " [-0.46359766 -0.03813995 -0.06968783 -0.4653723  -0.14574529  0.3870377\n",
      "  -0.65910083  0.1292486 ]\n",
      " [ 0.6605061  -0.19259916  0.00831191 -0.48197597  0.32038784  0.26782286\n",
      "   0.03419415  0.07020086]] , shape: (5, 8)\n"
     ]
    }
   ],
   "source": [
    "thalEmbedding = tf.feature_column.embedding_column(thalExample, dimension=8)\n",
    "inspect(thalEmbedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Hashed feature columns.\n",
    "<br>Use `hash_bucket_size` number of hash buckets to encode category strings. `hash_bucket_size` can be much smaller than the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] , shape: (5, 1000)\n"
     ]
    }
   ],
   "source": [
    "thalHashed = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    'thal', hash_bucket_size=1000)\n",
    "inspect(tf.feature_column.indicator_column(thalHashed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Crossed feature columns.\n",
    "<br>Hash encoding of **feature crosses**. The example below crosses the two features, age and thal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] , shape: (5, 1000)\n"
     ]
    }
   ],
   "source": [
    "featureCrossExample = tf.feature_column.crossed_column(\n",
    "    [ageBuckets, thalExample], hash_bucket_size=1000)\n",
    "inspect(tf.feature_column.indicator_column(featureCrossExample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
