{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "https://www.tensorflow.org/alpha/tutorials/text/word_embeddings\n",
    "\n",
    "### Ways of representing text as numbers\n",
    "We can encode each word in a text by\n",
    "- **A one-hot vector**&mdash;ineifficient because the representation vector will be unnecessarily sparse.\n",
    "- **A unique number**&mdash;because of its arbitrary representation, the integer encoding is hard to reflect the relationship between words.\n",
    "- **An embedding vector,** a (fixed-length) dense vector with learnable elements representing a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding layer can be set as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingLayer = keras.layers.Embedding(1000, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument of `Embedding`, `input_dim`, is the vocabulary size plus 1, and the second argument `output_dim` is the dimensionality of the embedding.\n",
    "\n",
    "The embedding layer maps word indices to dense vectors. An input should be a tensor of shape `(samples, sequenceLength)` with `int` elements, and then the output is a tensor of shape `(samples, sequenceLength, embeddingDimensionality)` with `float` elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "We will use the IMDB database and preprocess it like we've done previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularySize = 10000\n",
    "(trainData, trainLabels), (testData, testLabels) = keras.datasets.imdb.load_data(num_words=vocabularySize)\n",
    "\n",
    "wordToIdx = {word:(idx + 3)\n",
    "             for word,idx in keras.datasets.imdb.get_word_index().items()}\n",
    "wordToIdx[\"<PAD>\"] = 0\n",
    "wordToIdx[\"<START>\"] = 1\n",
    "wordToIdx[\"<UNK>\"] = 2  # unknown\n",
    "wordToIdx[\"<UNUSED>\"] = 3\n",
    "idxToWord = {value:key for key,value in wordToIdx.items()}\n",
    "\n",
    "# Padding\n",
    "maxLength = 500\n",
    "trainData = keras.preprocessing.sequence.pad_sequences(trainData,\n",
    "                                                       value=wordToIdx[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=maxLength)\n",
    "testData = keras.preprocessing.sequence.pad_sequences(testData,\n",
    "                                                      value=wordToIdx[\"<PAD>\"],\n",
    "                                                      padding='post',\n",
    "                                                      maxlen=maxLength)\n",
    "\n",
    "# Helper function for decoding integer encodings.\n",
    "def decodeReview(wordIdxs):\n",
    "    return ' '.join(idxToWord.get(idx, '?') for idx in wordIdxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500)\n",
      "(25000,)\n",
      "(25000, 500)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(trainData.shape)\n",
    "print(trainLabels.shape)\n",
    "print(testData.shape)\n",
    "print(testLabels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embeddingDim = 16\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocabularySize, embeddingDim, input_length=maxLength),\n",
    "    keras.layers.GlobalAvgPool1D(),\n",
    "    keras.layers.Dense(16, 'relu'),\n",
    "    keras.layers.Dense(1, 'sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "20000/20000 [==============================] - 2s 77us/sample - loss: 0.6917 - accuracy: 0.5544 - val_loss: 0.6898 - val_accuracy: 0.6424\n",
      "Epoch 2/30\n",
      "20000/20000 [==============================] - 1s 56us/sample - loss: 0.6862 - accuracy: 0.6230 - val_loss: 0.6814 - val_accuracy: 0.6158\n",
      "Epoch 3/30\n",
      "20000/20000 [==============================] - 1s 57us/sample - loss: 0.6746 - accuracy: 0.7261 - val_loss: 0.6650 - val_accuracy: 0.7548\n",
      "Epoch 4/30\n",
      "20000/20000 [==============================] - 1s 60us/sample - loss: 0.6528 - accuracy: 0.7638 - val_loss: 0.6369 - val_accuracy: 0.7628\n",
      "Epoch 5/30\n",
      "20000/20000 [==============================] - 1s 57us/sample - loss: 0.6186 - accuracy: 0.7932 - val_loss: 0.5980 - val_accuracy: 0.7878\n",
      "Epoch 6/30\n",
      "20000/20000 [==============================] - 1s 54us/sample - loss: 0.5741 - accuracy: 0.8095 - val_loss: 0.5524 - val_accuracy: 0.8062\n",
      "Epoch 7/30\n",
      "20000/20000 [==============================] - 1s 54us/sample - loss: 0.5258 - accuracy: 0.8238 - val_loss: 0.5079 - val_accuracy: 0.8210\n",
      "Epoch 8/30\n",
      "20000/20000 [==============================] - 1s 55us/sample - loss: 0.4774 - accuracy: 0.8461 - val_loss: 0.4669 - val_accuracy: 0.8342\n",
      "Epoch 9/30\n",
      "20000/20000 [==============================] - 1s 55us/sample - loss: 0.4342 - accuracy: 0.8605 - val_loss: 0.4310 - val_accuracy: 0.8500\n",
      "Epoch 10/30\n",
      "20000/20000 [==============================] - 1s 59us/sample - loss: 0.3965 - accuracy: 0.8727 - val_loss: 0.4027 - val_accuracy: 0.8598\n",
      "Epoch 11/30\n",
      "20000/20000 [==============================] - 1s 60us/sample - loss: 0.3655 - accuracy: 0.8796 - val_loss: 0.3796 - val_accuracy: 0.8648\n",
      "Epoch 12/30\n",
      "20000/20000 [==============================] - 1s 57us/sample - loss: 0.3401 - accuracy: 0.8857 - val_loss: 0.3623 - val_accuracy: 0.8672\n",
      "Epoch 13/30\n",
      "20000/20000 [==============================] - 1s 60us/sample - loss: 0.3185 - accuracy: 0.8928 - val_loss: 0.3476 - val_accuracy: 0.8696\n",
      "Epoch 14/30\n",
      "20000/20000 [==============================] - 1s 53us/sample - loss: 0.2999 - accuracy: 0.8993 - val_loss: 0.3359 - val_accuracy: 0.8722\n",
      "Epoch 15/30\n",
      "20000/20000 [==============================] - 1s 57us/sample - loss: 0.2854 - accuracy: 0.9036 - val_loss: 0.3267 - val_accuracy: 0.8776\n",
      "Epoch 16/30\n",
      "20000/20000 [==============================] - 1s 59us/sample - loss: 0.2712 - accuracy: 0.9069 - val_loss: 0.3247 - val_accuracy: 0.8734\n",
      "Epoch 17/30\n",
      "20000/20000 [==============================] - 1s 54us/sample - loss: 0.2590 - accuracy: 0.9112 - val_loss: 0.3127 - val_accuracy: 0.8818\n",
      "Epoch 18/30\n",
      "20000/20000 [==============================] - 1s 59us/sample - loss: 0.2473 - accuracy: 0.9151 - val_loss: 0.3062 - val_accuracy: 0.8826\n",
      "Epoch 19/30\n",
      "20000/20000 [==============================] - 1s 57us/sample - loss: 0.2372 - accuracy: 0.9187 - val_loss: 0.3017 - val_accuracy: 0.8824\n",
      "Epoch 20/30\n",
      "20000/20000 [==============================] - 1s 54us/sample - loss: 0.2278 - accuracy: 0.9209 - val_loss: 0.2980 - val_accuracy: 0.8858\n",
      "Epoch 21/30\n",
      "20000/20000 [==============================] - 1s 54us/sample - loss: 0.2199 - accuracy: 0.9239 - val_loss: 0.2942 - val_accuracy: 0.8868\n",
      "Epoch 22/30\n",
      "20000/20000 [==============================] - 1s 54us/sample - loss: 0.2135 - accuracy: 0.9262 - val_loss: 0.2916 - val_accuracy: 0.8886\n",
      "Epoch 23/30\n",
      "20000/20000 [==============================] - 1s 56us/sample - loss: 0.2053 - accuracy: 0.9298 - val_loss: 0.2905 - val_accuracy: 0.8890\n",
      "Epoch 24/30\n",
      "20000/20000 [==============================] - 1s 56us/sample - loss: 0.1982 - accuracy: 0.9317 - val_loss: 0.2873 - val_accuracy: 0.8912\n",
      "Epoch 25/30\n",
      "20000/20000 [==============================] - 1s 55us/sample - loss: 0.1915 - accuracy: 0.9336 - val_loss: 0.2868 - val_accuracy: 0.8898\n",
      "Epoch 26/30\n",
      "20000/20000 [==============================] - 1s 53us/sample - loss: 0.1864 - accuracy: 0.9354 - val_loss: 0.2854 - val_accuracy: 0.8898\n",
      "Epoch 27/30\n",
      "20000/20000 [==============================] - 1s 53us/sample - loss: 0.1806 - accuracy: 0.9387 - val_loss: 0.2851 - val_accuracy: 0.8908\n",
      "Epoch 28/30\n",
      "20000/20000 [==============================] - 1s 52us/sample - loss: 0.1748 - accuracy: 0.9402 - val_loss: 0.2829 - val_accuracy: 0.8916\n",
      "Epoch 29/30\n",
      "20000/20000 [==============================] - 1s 52us/sample - loss: 0.1692 - accuracy: 0.9437 - val_loss: 0.2823 - val_accuracy: 0.8916\n",
      "Epoch 30/30\n",
      "20000/20000 [==============================] - 1s 54us/sample - loss: 0.1644 - accuracy: 0.9451 - val_loss: 0.2845 - val_accuracy: 0.8922\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(trainData,\n",
    "                    trainLabels,\n",
    "                    epochs=30,\n",
    "                    batch_size=512,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the learned embeddings\n",
    "The learned embeddings will be a matrix of shape `(vocabSize, embeddingDim)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "embeddingW = model.layers[0].get_weights()[0]\n",
    "print(embeddingW.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot the embeddings using [**Embedding Projector**](http://projector.tensorflow.org/). Let's prepare tab separated files for the embedding vectors and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vecs.tsv', 'w', encoding='utf-8') as outV:\n",
    "    with open('meta.tsv', 'w', encoding='utf-8') as outM:\n",
    "        for idx in range(vocabularySize):\n",
    "            word = idxToWord[idx]\n",
    "            vector = embeddingW[idx]\n",
    "            outM.write(word + '\\n')\n",
    "            outV.write('\\t'.join(str(x) for x in vector) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the embeddings\n",
    "By uploading the prepared files to [Embedding Projector](http://projector.tensorflow.org/) through `Load`, we can see a 2D or 3D projection of the embeddings. From the projection, we can inspect which words are neighbors of a word of interest. The example below shows the neighbors of \"beautiful\".\n",
    "![](https://raw.githubusercontent.com/tensorflow/docs/master/site/en/r2/tutorials/text/images/embedding.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
