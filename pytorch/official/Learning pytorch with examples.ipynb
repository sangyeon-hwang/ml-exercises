{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Learning pytorch with examples.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"hvzn07lU2GIX","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0RnItQ-w2XmK","colab_type":"code","outputId":"573fe9f3-2163-494b-d510-b9ad5be1be8d","executionInfo":{"status":"ok","timestamp":1566386935514,"user_tz":-540,"elapsed":1881,"user":{"displayName":"Sang-Yeon Hwang","photoUrl":"","userId":"15582350424285819444"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["print(torch.__version__)\n","print(torch.cuda.get_device_name())\n","print(torch.cuda.is_available())"],"execution_count":2,"outputs":[{"output_type":"stream","text":["1.1.0\n","Tesla T4\n","True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L1HD7mF129Np","colab_type":"text"},"source":["## Two-layer network example: Manual learning"]},{"cell_type":"code","metadata":{"id":"J1UgCvAe4zAm","colab_type":"code","colab":{}},"source":["dtype = torch.float\n","device = torch.device('cuda')\n","\n","batchSize, dimIn, dimHidden, dimOut = 64, 1000, 100, 10\n","\n","# Random input and output data\n","X = torch.randn(batchSize, dimIn, device=device, dtype=dtype)\n","Y = torch.randn(batchSize, dimOut, device=device, dtype=dtype)\n","\n","# Randomly initialized weights\n","W1 = torch.randn(dimIn, dimHidden, device=device, dtype=dtype)\n","W2 = torch.randn(dimHidden, dimOut, device=device, dtype=dtype)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mudjo12ZZktl","colab_type":"code","outputId":"5d90b131-d25b-41ab-87bd-e6db0b88cedf","executionInfo":{"status":"ok","timestamp":1566386942242,"user_tz":-540,"elapsed":8602,"user":{"displayName":"Sang-Yeon Hwang","photoUrl":"","userId":"15582350424285819444"}},"colab":{"base_uri":"https://localhost:8080/","height":917}},"source":["lr = 1e-6\n","numEpochs = 500\n","for epoch in range(numEpochs):\n","    # Forward pass\n","    H = X.mm(W1)\n","    YPred = torch.relu(H).mm(W2)\n","    \n","    # Compute and print loss\n","    loss = (YPred - Y).pow(2).sum().item()\n","    if not epoch % 10:\n","        print(\"Epoch\", epoch, \", loss =\", loss)\n","    \n","    # Manual backprop\n","    gradYPred = 2.0 * (YPred - Y)\n","    gradW2 = torch.relu(H).t().mm(gradYPred)\n","    gradH = gradYPred.mm(W2.t())\n","    gradH[H < 0] = 0\n","    gradW1 = X.t().mm(gradH)\n","    \n","    # Update weights\n","    W1 -= lr * gradW1\n","    W2 -= lr * gradW2"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Epoch 0 , loss = 30110434.0\n","Epoch 10 , loss = 1881811.25\n","Epoch 20 , loss = 232516.0625\n","Epoch 30 , loss = 78475.6484375\n","Epoch 40 , loss = 32140.7578125\n","Epoch 50 , loss = 14692.7177734375\n","Epoch 60 , loss = 7237.71728515625\n","Epoch 70 , loss = 3731.4755859375\n","Epoch 80 , loss = 1986.7626953125\n","Epoch 90 , loss = 1084.8441162109375\n","Epoch 100 , loss = 604.2651977539062\n","Epoch 110 , loss = 341.5616760253906\n","Epoch 120 , loss = 195.31419372558594\n","Epoch 130 , loss = 112.7406234741211\n","Epoch 140 , loss = 65.587646484375\n","Epoch 150 , loss = 38.40731430053711\n","Epoch 160 , loss = 22.619701385498047\n","Epoch 170 , loss = 13.38724136352539\n","Epoch 180 , loss = 7.9582672119140625\n","Epoch 190 , loss = 4.74953556060791\n","Epoch 200 , loss = 2.8446455001831055\n","Epoch 210 , loss = 1.7094639539718628\n","Epoch 220 , loss = 1.0303103923797607\n","Epoch 230 , loss = 0.6226578950881958\n","Epoch 240 , loss = 0.3773045241832733\n","Epoch 250 , loss = 0.22920745611190796\n","Epoch 260 , loss = 0.13956552743911743\n","Epoch 270 , loss = 0.08517396450042725\n","Epoch 280 , loss = 0.05211341381072998\n","Epoch 290 , loss = 0.03195486590266228\n","Epoch 300 , loss = 0.019673632457852364\n","Epoch 310 , loss = 0.012165496125817299\n","Epoch 320 , loss = 0.007586466148495674\n","Epoch 330 , loss = 0.004791000857949257\n","Epoch 340 , loss = 0.003078926121816039\n","Epoch 350 , loss = 0.002025319030508399\n","Epoch 360 , loss = 0.0013676804956048727\n","Epoch 370 , loss = 0.0009499225416220725\n","Epoch 380 , loss = 0.0006769166793674231\n","Epoch 390 , loss = 0.0004945205873809755\n","Epoch 400 , loss = 0.00037093221908435225\n","Epoch 410 , loss = 0.00028493464924395084\n","Epoch 420 , loss = 0.00022320270363707095\n","Epoch 430 , loss = 0.00017758944886736572\n","Epoch 440 , loss = 0.00014394306344911456\n","Epoch 450 , loss = 0.00011774618906201795\n","Epoch 460 , loss = 9.84481448540464e-05\n","Epoch 470 , loss = 8.283433271571994e-05\n","Epoch 480 , loss = 7.052503497106954e-05\n","Epoch 490 , loss = 6.0899350501131266e-05\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i6YXXwB2bq6h","colab_type":"text"},"source":["## Learning with `autograd`"]},{"cell_type":"code","metadata":{"id":"3fvNPXnWbzMH","colab_type":"code","outputId":"ead3f911-1451-4a09-e977-29efd289c16e","executionInfo":{"status":"ok","timestamp":1566386942498,"user_tz":-540,"elapsed":8853,"user":{"displayName":"Sang-Yeon Hwang","photoUrl":"","userId":"15582350424285819444"}},"colab":{"base_uri":"https://localhost:8080/","height":917}},"source":["W1 = torch.randn(dimIn, dimHidden, device=device, dtype=dtype,\n","                 requires_grad=True)\n","W2 = torch.randn(dimHidden, dimOut, device=device, dtype=dtype,\n","                 requires_grad=True)\n","\n","# X and Y are reused because they were automatically set `requires_grad=False`.\n","\n","lr = 1e-6\n","numEpochs = 500\n","for epoch in range(numEpochs):\n","    # Forward\n","    YPred = torch.relu(X.mm(W1)).mm(W2)\n","    \n","    # Loss: now a Tensor of shape (1,).\n","    # loss.item() gets the scalar value.\n","    loss = (YPred - Y).pow(2).sum()\n","    if not epoch % 10:\n","        print(\"Epoch\", epoch, \", loss =\", loss.item())\n","    \n","    # Backward: compute `W1.grad` and `W2.grad`, both being Tensors\n","    # holding the gradient of loss on W1 and W2.\n","    loss.backward()\n","    \n","    # Manual update.\n","    # We don't want this \"update\" process to be also tracked.\n","    with torch.no_grad():\n","        W1 -= lr * W1.grad\n","        W2 -= lr * W2.grad\n","        \n","        # Manually zero the gradients after the update.\n","        W1.grad.zero_()\n","        W2.grad.zero_()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Epoch 0 , loss = 41813200.0\n","Epoch 10 , loss = 953174.3125\n","Epoch 20 , loss = 229398.171875\n","Epoch 30 , loss = 79504.2734375\n","Epoch 40 , loss = 32678.109375\n","Epoch 50 , loss = 14811.6865234375\n","Epoch 60 , loss = 7142.232421875\n","Epoch 70 , loss = 3595.380859375\n","Epoch 80 , loss = 1865.2724609375\n","Epoch 90 , loss = 989.3023681640625\n","Epoch 100 , loss = 533.7219848632812\n","Epoch 110 , loss = 291.90008544921875\n","Epoch 120 , loss = 161.42820739746094\n","Epoch 130 , loss = 90.09970092773438\n","Epoch 140 , loss = 50.67060470581055\n","Epoch 150 , loss = 28.68417739868164\n","Epoch 160 , loss = 16.32929801940918\n","Epoch 170 , loss = 9.339933395385742\n","Epoch 180 , loss = 5.364675521850586\n","Epoch 190 , loss = 3.0928311347961426\n","Epoch 200 , loss = 1.7889349460601807\n","Epoch 210 , loss = 1.0377182960510254\n","Epoch 220 , loss = 0.6035274863243103\n","Epoch 230 , loss = 0.35177552700042725\n","Epoch 240 , loss = 0.20550459623336792\n","Epoch 250 , loss = 0.12023797631263733\n","Epoch 260 , loss = 0.07052139937877655\n","Epoch 270 , loss = 0.04139437526464462\n","Epoch 280 , loss = 0.024381861090660095\n","Epoch 290 , loss = 0.014428442344069481\n","Epoch 300 , loss = 0.008595069870352745\n","Epoch 310 , loss = 0.0051836660131812096\n","Epoch 320 , loss = 0.0031826095655560493\n","Epoch 330 , loss = 0.002009342424571514\n","Epoch 340 , loss = 0.0013094393070787191\n","Epoch 350 , loss = 0.0008828053250908852\n","Epoch 360 , loss = 0.000613731041084975\n","Epoch 370 , loss = 0.0004405454092193395\n","Epoch 380 , loss = 0.0003236480406485498\n","Epoch 390 , loss = 0.00024511999799869955\n","Epoch 400 , loss = 0.00018865762103814632\n","Epoch 410 , loss = 0.0001503560779383406\n","Epoch 420 , loss = 0.00012074209371348843\n","Epoch 430 , loss = 9.954952838597819e-05\n","Epoch 440 , loss = 8.236414578277618e-05\n","Epoch 450 , loss = 6.948609370738268e-05\n","Epoch 460 , loss = 5.942819552728906e-05\n","Epoch 470 , loss = 5.15182655362878e-05\n","Epoch 480 , loss = 4.4940610678168014e-05\n","Epoch 490 , loss = 3.993579593952745e-05\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5XthwB3GTz5U","colab_type":"text"},"source":["## Custom `autograd`"]},{"cell_type":"code","metadata":{"id":"gmzTp5WTT9xt","colab_type":"code","outputId":"c433e035-67f5-45e8-887c-cd478a2ad0fd","executionInfo":{"status":"ok","timestamp":1566386943118,"user_tz":-540,"elapsed":9469,"user":{"displayName":"Sang-Yeon Hwang","photoUrl":"","userId":"15582350424285819444"}},"colab":{"base_uri":"https://localhost:8080/","height":917}},"source":["class MyReLU(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, input_):\n","        \"\"\"Compute an ouput Tensor from an input Tensor.\n","        \n","        ctx: a context object for stashing information in backward pass.\n","        \"\"\"\n","        ctx.save_for_backward(input_)\n","        return torch.relu(input_)\n","    \n","    @staticmethod\n","    def backward(ctx, gradOutput):\n","        \"\"\"Compute the grad of loss wrt input.\n","        \n","        gradOutput: a Tensor containing the grad of loss wrt output.\n","        \"\"\"\n","        input_, = ctx.saved_tensors\n","        gradInput = gradOutput.clone()\n","        gradInput[input_ < 0] = 0  # ReLU gradient\n","        return gradInput\n","\n","W1 = torch.randn(dimIn, dimHidden, device=device, dtype=dtype,\n","                 requires_grad=True)\n","W2 = torch.randn(dimHidden, dimOut, device=device, dtype=dtype,\n","                 requires_grad=True)\n","\n","lr = 1e-6\n","numEpochs = 500\n","for epoch in range(numEpochs):\n","    # Forward\n","    YPred = MyReLU.apply(X.mm(W1)).mm(W2)\n","    \n","    # Loss\n","    loss = (YPred - Y).pow(2).sum()\n","    if not epoch % 10:\n","        print(\"Epoch\", epoch, \", loss:\", loss.item())\n","    \n","    # Use our autograd to compute the loss grad wrt input.\n","    loss.backward()\n","    \n","    # Update the weights.\n","    with torch.no_grad():\n","        W1 -= lr * W1.grad\n","        W2 -= lr * W2.grad\n","        W1.grad.zero_()\n","        W2.grad.zero_()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Epoch 0 , loss: 37581768.0\n","Epoch 10 , loss: 996981.25\n","Epoch 20 , loss: 220346.15625\n","Epoch 30 , loss: 77763.125\n","Epoch 40 , loss: 33184.27734375\n","Epoch 50 , loss: 15686.3330078125\n","Epoch 60 , loss: 7895.3759765625\n","Epoch 70 , loss: 4162.25\n","Epoch 80 , loss: 2274.115234375\n","Epoch 90 , loss: 1279.881591796875\n","Epoch 100 , loss: 739.3388671875\n","Epoch 110 , loss: 437.3294677734375\n","Epoch 120 , loss: 263.8274841308594\n","Epoch 130 , loss: 161.9019775390625\n","Epoch 140 , loss: 100.83695983886719\n","Epoch 150 , loss: 63.615699768066406\n","Epoch 160 , loss: 40.57720184326172\n","Epoch 170 , loss: 26.128400802612305\n","Epoch 180 , loss: 16.96102523803711\n","Epoch 190 , loss: 11.088306427001953\n","Epoch 200 , loss: 7.297678470611572\n","Epoch 210 , loss: 4.827805519104004\n","Epoch 220 , loss: 3.2080435752868652\n","Epoch 230 , loss: 2.140023708343506\n","Epoch 240 , loss: 1.4322056770324707\n","Epoch 250 , loss: 0.9614217281341553\n","Epoch 260 , loss: 0.6469417810440063\n","Epoch 270 , loss: 0.4363449215888977\n","Epoch 280 , loss: 0.29484623670578003\n","Epoch 290 , loss: 0.19963587820529938\n","Epoch 300 , loss: 0.13537411391735077\n","Epoch 310 , loss: 0.09194529801607132\n","Epoch 320 , loss: 0.06256075948476791\n","Epoch 330 , loss: 0.04263302683830261\n","Epoch 340 , loss: 0.029100682586431503\n","Epoch 350 , loss: 0.019931035116314888\n","Epoch 360 , loss: 0.01367931067943573\n","Epoch 370 , loss: 0.009458261542022228\n","Epoch 380 , loss: 0.00658147968351841\n","Epoch 390 , loss: 0.004613256547600031\n","Epoch 400 , loss: 0.003271002322435379\n","Epoch 410 , loss: 0.002353125950321555\n","Epoch 420 , loss: 0.0017193856183439493\n","Epoch 430 , loss: 0.0012755831703543663\n","Epoch 440 , loss: 0.0009611209388822317\n","Epoch 450 , loss: 0.0007343582110479474\n","Epoch 460 , loss: 0.000569421739783138\n","Epoch 470 , loss: 0.0004477525071706623\n","Epoch 480 , loss: 0.00035858008777722716\n","Epoch 490 , loss: 0.00029013119637966156\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uyHoctP7r4eF","colab_type":"text"},"source":["## Using `torch.nn`"]},{"cell_type":"code","metadata":{"id":"MjaasO-IlpLw","colab_type":"code","outputId":"1213671b-2e1e-458a-f9cf-46e0316811a2","executionInfo":{"status":"ok","timestamp":1566386943627,"user_tz":-540,"elapsed":9973,"user":{"displayName":"Sang-Yeon Hwang","photoUrl":"","userId":"15582350424285819444"}},"colab":{"base_uri":"https://localhost:8080/","height":917}},"source":["model = torch.nn.Sequential(\n","    torch.nn.Linear(dimIn, dimHidden),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(dimHidden, dimOut)\n",").to(device)\n","\n","lossFunc = torch.nn.MSELoss(reduction='sum')\n","\n","lr = 1e-4\n","numEpochs = 500\n","for epoch in range(numEpochs):\n","    # Forward\n","    YPred = model(X)\n","    \n","    # Loss\n","    loss = lossFunc(YPred, Y)\n","    if not epoch % 10:\n","        print(\"Epoch\", epoch, \", loss:\", loss.item())\n","    \n","    # Zero the gradients before backward pass.\n","    model.zero_grad()\n","    \n","    # Backward.\n","    # Compute the loss grad wrt the parameter Tensors of `model`\n","    # that were automatically stored with `requires_grad=True`.\n","    loss.backward()\n","    \n","    # Update\n","    with torch.no_grad():\n","        for param in model.parameters():\n","            param -= lr * param.grad"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Epoch 0 , loss: 667.0211181640625\n","Epoch 10 , loss: 372.93719482421875\n","Epoch 20 , loss: 225.90145874023438\n","Epoch 30 , loss: 132.2462921142578\n","Epoch 40 , loss: 75.00849914550781\n","Epoch 50 , loss: 42.240482330322266\n","Epoch 60 , loss: 24.232006072998047\n","Epoch 70 , loss: 14.303144454956055\n","Epoch 80 , loss: 8.689802169799805\n","Epoch 90 , loss: 5.4155497550964355\n","Epoch 100 , loss: 3.4450552463531494\n","Epoch 110 , loss: 2.227128505706787\n","Epoch 120 , loss: 1.4571682214736938\n","Epoch 130 , loss: 0.9619020223617554\n","Epoch 140 , loss: 0.6394496560096741\n","Epoch 150 , loss: 0.42782217264175415\n","Epoch 160 , loss: 0.2882079482078552\n","Epoch 170 , loss: 0.19537928700447083\n","Epoch 180 , loss: 0.13322681188583374\n","Epoch 190 , loss: 0.0913565456867218\n","Epoch 200 , loss: 0.06296852231025696\n","Epoch 210 , loss: 0.04365168884396553\n","Epoch 220 , loss: 0.0304095558822155\n","Epoch 230 , loss: 0.021283671259880066\n","Epoch 240 , loss: 0.014960355125367641\n","Epoch 250 , loss: 0.01055900938808918\n","Epoch 260 , loss: 0.0074802786111831665\n","Epoch 270 , loss: 0.005317033268511295\n","Epoch 280 , loss: 0.0037914824206382036\n","Epoch 290 , loss: 0.002711379900574684\n","Epoch 300 , loss: 0.001943929702974856\n","Epoch 310 , loss: 0.0013970575528219342\n","Epoch 320 , loss: 0.0010062074288725853\n","Epoch 330 , loss: 0.0007270036730915308\n","Epoch 340 , loss: 0.0005263082566671073\n","Epoch 350 , loss: 0.0003816442913375795\n","Epoch 360 , loss: 0.0002771716099232435\n","Epoch 370 , loss: 0.00020156170648988336\n","Epoch 380 , loss: 0.00014678640582133085\n","Epoch 390 , loss: 0.00010700879647629336\n","Epoch 400 , loss: 7.809440285200253e-05\n","Epoch 410 , loss: 5.704887007595971e-05\n","Epoch 420 , loss: 4.171114051132463e-05\n","Epoch 430 , loss: 3.051995372516103e-05\n","Epoch 440 , loss: 2.2352425730787218e-05\n","Epoch 450 , loss: 1.6379117369069718e-05\n","Epoch 460 , loss: 1.2010465070488863e-05\n","Epoch 470 , loss: 8.812560736259911e-06\n","Epoch 480 , loss: 6.470260814239737e-06\n","Epoch 490 , loss: 4.752016593556618e-06\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2Y0yiXy-8PFK","colab_type":"text"},"source":["## Using `optim`"]},{"cell_type":"code","metadata":{"id":"Q8C9owM58ofU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":917},"outputId":"576c3887-0a2e-40c6-b7c6-46d11c31f21b","executionInfo":{"status":"ok","timestamp":1566387518655,"user_tz":-540,"elapsed":1278,"user":{"displayName":"Sang-Yeon Hwang","photoUrl":"","userId":"15582350424285819444"}}},"source":["model = torch.nn.Sequential(\n","    torch.nn.Linear(dimIn, dimHidden),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(dimHidden, dimOut)\n",").to(device)\n","\n","lossFunc = torch.nn.MSELoss(reduction='sum')\n","\n","lr = 1e-4\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","numEpochs = 500\n","for epoch in range(numEpochs):\n","    # Forward\n","    YPred = model(X)\n","    \n","    # Loss\n","    loss = lossFunc(YPred, Y)\n","    if not epoch % 10:\n","        print(\"Epoch\", epoch, \", loss:\", loss.item())\n","\n","    # Zero all the gradients.\n","    # By default, `backward` accumulates gradients rather than overwites them.\n","    optimizer.zero_grad()\n","    \n","    # Backward\n","    loss.backward()\n","    \n","    # Update the parameters.\n","    optimizer.step()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Epoch 0 , loss: 666.832763671875\n","Epoch 10 , loss: 515.1885375976562\n","Epoch 20 , loss: 402.29302978515625\n","Epoch 30 , loss: 315.60009765625\n","Epoch 40 , loss: 248.55567932128906\n","Epoch 50 , loss: 194.6269073486328\n","Epoch 60 , loss: 150.83323669433594\n","Epoch 70 , loss: 115.55943298339844\n","Epoch 80 , loss: 87.15130615234375\n","Epoch 90 , loss: 64.56743621826172\n","Epoch 100 , loss: 46.84429168701172\n","Epoch 110 , loss: 33.12757873535156\n","Epoch 120 , loss: 22.792102813720703\n","Epoch 130 , loss: 15.274070739746094\n","Epoch 140 , loss: 10.000965118408203\n","Epoch 150 , loss: 6.410922527313232\n","Epoch 160 , loss: 4.0355095863342285\n","Epoch 170 , loss: 2.5035464763641357\n","Epoch 180 , loss: 1.5394915342330933\n","Epoch 190 , loss: 0.945873498916626\n","Epoch 200 , loss: 0.5833356380462646\n","Epoch 210 , loss: 0.3631114363670349\n","Epoch 220 , loss: 0.2295474112033844\n","Epoch 230 , loss: 0.14826789498329163\n","Epoch 240 , loss: 0.09816903620958328\n","Epoch 250 , loss: 0.06667209416627884\n","Epoch 260 , loss: 0.04640299081802368\n","Epoch 270 , loss: 0.032986223697662354\n","Epoch 280 , loss: 0.023848144337534904\n","Epoch 290 , loss: 0.017439696937799454\n","Epoch 300 , loss: 0.012842850759625435\n","Epoch 310 , loss: 0.009487863630056381\n","Epoch 320 , loss: 0.007010667119175196\n","Epoch 330 , loss: 0.005169479176402092\n","Epoch 340 , loss: 0.003797776997089386\n","Epoch 350 , loss: 0.002776521258056164\n","Epoch 360 , loss: 0.0020183546002954245\n","Epoch 370 , loss: 0.0014579769922420382\n","Epoch 380 , loss: 0.0010461745550855994\n","Epoch 390 , loss: 0.0007455295417457819\n","Epoch 400 , loss: 0.0005275381845422089\n","Epoch 410 , loss: 0.000370627676602453\n","Epoch 420 , loss: 0.00025851710233837366\n","Epoch 430 , loss: 0.0001790196110960096\n","Epoch 440 , loss: 0.00012307266297284514\n","Epoch 450 , loss: 8.399850776186213e-05\n","Epoch 460 , loss: 5.691426122211851e-05\n","Epoch 470 , loss: 3.828699482255615e-05\n","Epoch 480 , loss: 2.5566831027390435e-05\n","Epoch 490 , loss: 1.6948240954661742e-05\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JE09uBzb-EYW","colab_type":"text"},"source":["## Custom `nn`\n","\n","For more complex models."]},{"cell_type":"code","metadata":{"id":"o5-aT9Ag-TSd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":917},"outputId":"513eadc3-237b-43d4-f3bf-ebfd3f3f54dc","executionInfo":{"status":"ok","timestamp":1566389875379,"user_tz":-540,"elapsed":1082,"user":{"displayName":"Sang-Yeon Hwang","photoUrl":"","userId":"15582350424285819444"}}},"source":["class TwoLayerNet(torch.nn.Module):\n","    def __init__(self, dimIn, dimHidden, dimOut):\n","        super().__init__()\n","        self.linear1 = torch.nn.Linear(dimIn, dimHidden)\n","        self.linear2 = torch.nn.Linear(dimHidden, dimOut)\n","        \n","    def forward(self, X):\n","        \"\"\"\n","        `forward` must accept an input Tensor and return an output Tensor.\n","        \"\"\"\n","        YPred = self.linear2(torch.relu(self.linear1(X)))\n","        return YPred\n","\n","model = TwoLayerNet(dimIn, dimHidden, dimOut).to(device)\n","lossFunc = torch.nn.MSELoss(reduction='sum')\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n","for epoch in range(500):\n","    # Forward\n","    YPred = model(X)\n","    \n","    # Loss\n","    loss = lossFunc(YPred, Y)\n","    if not epoch % 10:\n","        print(\"Epoch\", epoch, \", loss:\", loss.item())\n","    \n","    # Backward and update\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Epoch 0 , loss: 645.6306762695312\n","Epoch 10 , loss: 359.6498107910156\n","Epoch 20 , loss: 208.09262084960938\n","Epoch 30 , loss: 111.8485107421875\n","Epoch 40 , loss: 57.78961944580078\n","Epoch 50 , loss: 30.421998977661133\n","Epoch 60 , loss: 16.76714515686035\n","Epoch 70 , loss: 9.630976676940918\n","Epoch 80 , loss: 5.703324317932129\n","Epoch 90 , loss: 3.4544947147369385\n","Epoch 100 , loss: 2.1289784908294678\n","Epoch 110 , loss: 1.3320127725601196\n","Epoch 120 , loss: 0.8451871275901794\n","Epoch 130 , loss: 0.5428599119186401\n","Epoch 140 , loss: 0.3532104790210724\n","Epoch 150 , loss: 0.23301978409290314\n","Epoch 160 , loss: 0.15587753057479858\n","Epoch 170 , loss: 0.10552483052015305\n","Epoch 180 , loss: 0.07225219160318375\n","Epoch 190 , loss: 0.05001062527298927\n","Epoch 200 , loss: 0.03493734449148178\n","Epoch 210 , loss: 0.024613944813609123\n","Epoch 220 , loss: 0.017473118379712105\n","Epoch 230 , loss: 0.012489109300076962\n","Epoch 240 , loss: 0.008983037434518337\n","Epoch 250 , loss: 0.0064963968470692635\n","Epoch 260 , loss: 0.004720793105661869\n","Epoch 270 , loss: 0.003445572918280959\n","Epoch 280 , loss: 0.002523735398426652\n","Epoch 290 , loss: 0.0018549029482528567\n","Epoch 300 , loss: 0.0013675850350409746\n","Epoch 310 , loss: 0.0010110476287081838\n","Epoch 320 , loss: 0.0007493158336728811\n","Epoch 330 , loss: 0.0005565122119151056\n","Epoch 340 , loss: 0.0004141491081099957\n","Epoch 350 , loss: 0.0003095648717135191\n","Epoch 360 , loss: 0.00023186817998066545\n","Epoch 370 , loss: 0.00017395432223565876\n","Epoch 380 , loss: 0.00013069227861706167\n","Epoch 390 , loss: 9.830926137510687e-05\n","Epoch 400 , loss: 7.402689516311511e-05\n","Epoch 410 , loss: 5.5797252571210265e-05\n","Epoch 420 , loss: 4.209198959870264e-05\n","Epoch 430 , loss: 3.177529652020894e-05\n","Epoch 440 , loss: 2.400490120635368e-05\n","Epoch 450 , loss: 1.814469214878045e-05\n","Epoch 460 , loss: 1.3723293704970274e-05\n","Epoch 470 , loss: 1.0384496818005573e-05\n","Epoch 480 , loss: 7.861647645768244e-06\n","Epoch 490 , loss: 5.954111202299828e-06\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4Qh7cDrhHHJY","colab_type":"text"},"source":["## Control flow + weight sharing: An example of dynamic graphs\n","\n","For each forward, use a shared-weight hidden layer for a randomly chosen number of times."]},{"cell_type":"code","metadata":{"id":"KhVqU6LhHXse","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":917},"outputId":"ec0900bf-34dc-4919-a3ce-de0a95e3d37c","executionInfo":{"status":"ok","timestamp":1566390750456,"user_tz":-540,"elapsed":1221,"user":{"displayName":"Sang-Yeon Hwang","photoUrl":"","userId":"15582350424285819444"}}},"source":["import random\n","\n","class DynamicNet(torch.nn.Module):\n","    def __init__(self, dimIn, dimHidden, dimOut):\n","        super().__init__()\n","        self.inLinear = torch.nn.Linear(dimIn, dimHidden)\n","        self.midLinear = torch.nn.Linear(dimHidden, dimHidden)\n","        self.outLinear = torch.nn.Linear(dimHidden, dimOut)\n","        \n","    def forward(self, X):\n","        reluH = self.inLinear(X).clamp(min=0)  # clamp(0) == relu\n","        for _ in range(random.randint(0, 3)):\n","            reluH = self.midLinear(reluH).clamp(min=0)\n","        YPred = self.outLinear(reluH)\n","        return YPred\n","\n","model = DynamicNet(dimIn, dimHidden, dimOut).to(device)\n","lossFunc = torch.nn.MSELoss(reduction='sum')\n","# The current dynamic network is hard to be trained with vanilla SGD,\n","# so we use momentum.\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n","for epoch in range(500):\n","    # Forward\n","    YPred = model(X)\n","    \n","    # Loss\n","    loss = lossFunc(YPred, Y)\n","    if not epoch % 10:\n","        print(\"Epoch\", epoch, \", loss:\", loss.item())\n","        \n","    # Backward and update\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Epoch 0 , loss: 625.861083984375\n","Epoch 10 , loss: 570.870361328125\n","Epoch 20 , loss: 565.5037841796875\n","Epoch 30 , loss: 351.8943786621094\n","Epoch 40 , loss: 99.09777069091797\n","Epoch 50 , loss: 125.12979125976562\n","Epoch 60 , loss: 170.66844177246094\n","Epoch 70 , loss: 111.08098602294922\n","Epoch 80 , loss: 30.483501434326172\n","Epoch 90 , loss: 11.141931533813477\n","Epoch 100 , loss: 17.901830673217773\n","Epoch 110 , loss: 17.76694679260254\n","Epoch 120 , loss: 20.74614715576172\n","Epoch 130 , loss: 18.20951271057129\n","Epoch 140 , loss: 13.469329833984375\n","Epoch 150 , loss: 4.302995204925537\n","Epoch 160 , loss: 3.0961363315582275\n","Epoch 170 , loss: 9.583621978759766\n","Epoch 180 , loss: 2.4331865310668945\n","Epoch 190 , loss: 2.3768906593322754\n","Epoch 200 , loss: 0.8927303552627563\n","Epoch 210 , loss: 8.006782531738281\n","Epoch 220 , loss: 1.1096906661987305\n","Epoch 230 , loss: 1.0338538885116577\n","Epoch 240 , loss: 1.7363452911376953\n","Epoch 250 , loss: 20.106325149536133\n","Epoch 260 , loss: 4.408821105957031\n","Epoch 270 , loss: 17.523386001586914\n","Epoch 280 , loss: 7.0079779624938965\n","Epoch 290 , loss: 0.7332276105880737\n","Epoch 300 , loss: 16.605175018310547\n","Epoch 310 , loss: 11.649577140808105\n","Epoch 320 , loss: 5.851512432098389\n","Epoch 330 , loss: 36.45026397705078\n","Epoch 340 , loss: 6.062655448913574\n","Epoch 350 , loss: 2.777390241622925\n","Epoch 360 , loss: 4.016909599304199\n","Epoch 370 , loss: 15.347190856933594\n","Epoch 380 , loss: 7.000997066497803\n","Epoch 390 , loss: 0.5130802392959595\n","Epoch 400 , loss: 3.588905096054077\n","Epoch 410 , loss: 5.887477874755859\n","Epoch 420 , loss: 2.8131608963012695\n","Epoch 430 , loss: 0.7200207114219666\n","Epoch 440 , loss: 0.5006309151649475\n","Epoch 450 , loss: 0.3253166079521179\n","Epoch 460 , loss: 2.7919554710388184\n","Epoch 470 , loss: 1.677638053894043\n","Epoch 480 , loss: 0.13728046417236328\n","Epoch 490 , loss: 0.11643567681312561\n"],"name":"stdout"}]}]}